{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qheRrKRKTfW"
      },
      "source": [
        "# Import library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lumPhm9eKPSI"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, Conv1D, GlobalMaxPooling1D, Dense\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUQDaZHawmmT"
      },
      "source": [
        "# Fungsi Membaca file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMeFLiinbP6d"
      },
      "outputs": [],
      "source": [
        "def read_file(file_name):\n",
        "    data_list  = []\n",
        "    with open(file_name, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n",
        "            text = line[line.find(\"]\")+1:].strip()\n",
        "            data_list.append([label, text])\n",
        "    return data_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjOWyGQbI501"
      },
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jyr515j-ivyN",
        "outputId": "fee94ea5-0967-4cd6-e371-b44bf87d4099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3cxAaWXJBaA"
      },
      "source": [
        "# Read file dari GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMLueValipjR"
      },
      "outputs": [],
      "source": [
        "# file_name = \"olympic.txt\"\n",
        "file_name = \"/content/drive/MyDrive/Colab Notebooks/text2emoji-master/data/psychExp.txt\"\n",
        "psychExp_txt = read_file(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak2vWOO8JANZ"
      },
      "source": [
        "# Print jumlah data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsD4Vo4LiyIY",
        "outputId": "5f9c1af9-d1c1-47f3-e821-600f4eb4fc80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of instances: 7480\n"
          ]
        }
      ],
      "source": [
        "print(\"The number of instances: {}\".format(len(psychExp_txt)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmXcmtYpJPYF"
      },
      "source": [
        "# Membaca contoh data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVRQ25gri059",
        "outputId": "88e7f551-36d9-45c2-a692-bd720d88d1a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data example: \n",
            "['1. 0. 0. 0. 0. 0. 0.', 'During the period of falling in love, each time that we met and especially when we had not met for a long time.']\n"
          ]
        }
      ],
      "source": [
        "print(\"Data example: \")\n",
        "print(psychExp_txt[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oX3vYISJr5j"
      },
      "source": [
        "# Membuat Fitur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MJQebY4i3aD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4XTu_shi5iC"
      },
      "outputs": [],
      "source": [
        "def ngram(token, n):\n",
        "    output = []\n",
        "    for i in range(n-1, len(token)):\n",
        "        ngram = ' '.join(token[i-n+1:i+1])\n",
        "        output.append(ngram)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCO6VNmWi7do"
      },
      "outputs": [],
      "source": [
        "def create_feature(text, nrange=(1, 1)):\n",
        "    text_features = []\n",
        "    text = text.lower()\n",
        "\n",
        "    # 1. treat alphanumeric characters as word tokens\n",
        "    # Since tweets contain #, we keep it as a feature\n",
        "    # Then, extract all ngram lengths\n",
        "    text_alphanum = re.sub('[^a-z0-9#]', ' ', text)\n",
        "    for n in range(nrange[0], nrange[1]+1):\n",
        "        text_features += ngram(text_alphanum.split(), n)\n",
        "\n",
        "    # 2. treat punctuations as word token\n",
        "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
        "    text_features += ngram(text_punc.split(), 1)\n",
        "\n",
        "    # 3. Return a dictinaory whose keys are the list of elements\n",
        "    # and their values are the number of times appearede in the list.\n",
        "    return Counter(text_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjtoq4IfJw3s"
      },
      "source": [
        "# Print Fitur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXYnY-_Qi9p_",
        "outputId": "4d2b3d09-dba4-475c-a65d-444468d9340e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({'i': 1, 'love': 1, 'you': 1, '!': 1})\n",
            "Counter({'aly': 1, 'wins': 1, 'the': 1, 'gold': 1, '#olympics': 1, '!!!!!!': 1, '#': 1})\n",
            "Counter({'aly': 1, 'wins': 1, 'the': 1, 'gold': 1, '#olympics': 1, 'aly wins': 1, 'wins the': 1, 'the gold': 1, 'gold #olympics': 1, '!!!!!!': 1, '#': 1})\n"
          ]
        }
      ],
      "source": [
        "print(create_feature(\"I love you!\"))\n",
        "print(create_feature(\" aly wins the gold!!!!!!  #olympics\"))\n",
        "print(create_feature(\" aly wins the gold!!!!!!  #olympics\", (1, 2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzfQFi7bJ0xp"
      },
      "source": [
        "# Membuat fungsi konversi label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pagr_b4ajAGF"
      },
      "outputs": [],
      "source": [
        "def convert_label(item, name):\n",
        "    items = list(map(float, item.split()))\n",
        "    label = \"\"\n",
        "    for idx in range(len(items)):\n",
        "        if items[idx] == 1:\n",
        "            label += name[idx] + \" \"\n",
        "\n",
        "    return label.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KFuNj_YjCtt"
      },
      "outputs": [],
      "source": [
        "emotions = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
        "\n",
        "X_all = []\n",
        "y_all = []\n",
        "for label, text in psychExp_txt:\n",
        "    y_all.append(convert_label(label, emotions))\n",
        "    X_all.append(create_feature(text, nrange=(1, 4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_8eZf58J-0r"
      },
      "source": [
        "# Print hasil Konversi Label dan contoh fitur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-zpCEBhjEmf",
        "outputId": "3d40e939-07cc-41d9-fa60-c7c758ee8b55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "features example: \n",
            "Counter({'time': 2, 'we': 2, 'met': 2, 'during': 1, 'the': 1, 'period': 1, 'of': 1, 'falling': 1, 'in': 1, 'love': 1, 'each': 1, 'that': 1, 'and': 1, 'especially': 1, 'when': 1, 'had': 1, 'not': 1, 'for': 1, 'a': 1, 'long': 1, 'during the': 1, 'the period': 1, 'period of': 1, 'of falling': 1, 'falling in': 1, 'in love': 1, 'love each': 1, 'each time': 1, 'time that': 1, 'that we': 1, 'we met': 1, 'met and': 1, 'and especially': 1, 'especially when': 1, 'when we': 1, 'we had': 1, 'had not': 1, 'not met': 1, 'met for': 1, 'for a': 1, 'a long': 1, 'long time': 1, 'during the period': 1, 'the period of': 1, 'period of falling': 1, 'of falling in': 1, 'falling in love': 1, 'in love each': 1, 'love each time': 1, 'each time that': 1, 'time that we': 1, 'that we met': 1, 'we met and': 1, 'met and especially': 1, 'and especially when': 1, 'especially when we': 1, 'when we had': 1, 'we had not': 1, 'had not met': 1, 'not met for': 1, 'met for a': 1, 'for a long': 1, 'a long time': 1, 'during the period of': 1, 'the period of falling': 1, 'period of falling in': 1, 'of falling in love': 1, 'falling in love each': 1, 'in love each time': 1, 'love each time that': 1, 'each time that we': 1, 'time that we met': 1, 'that we met and': 1, 'we met and especially': 1, 'met and especially when': 1, 'and especially when we': 1, 'especially when we had': 1, 'when we had not': 1, 'we had not met': 1, 'had not met for': 1, 'not met for a': 1, 'met for a long': 1, 'for a long time': 1, ',': 1, '.': 1})\n"
          ]
        }
      ],
      "source": [
        "print(\"features example: \")\n",
        "print(X_all[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkR9elK6jGrO",
        "outputId": "8c4b8137-370a-4747-f325-d1fba4242878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label example:\n",
            "joy\n"
          ]
        }
      ],
      "source": [
        "print(\"Label example:\")\n",
        "print(y_all[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4eVl22WMszn"
      },
      "source": [
        "# Label Emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujsaWZ55kU5k"
      },
      "outputs": [],
      "source": [
        "emoji_dict = {\"joy\":\"😂\", \"fear\":\"😱\", \"anger\":\"😠\", \"sadness\":\"😢\", \"disgust\":\"😒\", \"shame\":\"😳\", \"guilt\":\"😳\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmC0eWcCLxrH"
      },
      "source": [
        "# Training dan testing dengan CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpUoO8ECyRmg",
        "outputId": "c366296e-4d84-4ec0-e06c-d2aeb8bd56e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "169/169 [==============================] - 3s 11ms/step - loss: 1.8793 - accuracy: 0.2579 - val_loss: 1.5732 - val_accuracy: 0.4908\n",
            "Epoch 2/10\n",
            "169/169 [==============================] - 2s 10ms/step - loss: 1.2648 - accuracy: 0.5768 - val_loss: 1.1296 - val_accuracy: 0.6060\n",
            "Epoch 3/10\n",
            "169/169 [==============================] - 2s 11ms/step - loss: 0.8048 - accuracy: 0.7435 - val_loss: 1.1066 - val_accuracy: 0.6427\n",
            "Epoch 4/10\n",
            "169/169 [==============================] - 2s 10ms/step - loss: 0.4458 - accuracy: 0.8758 - val_loss: 1.1266 - val_accuracy: 0.6327\n",
            "Epoch 5/10\n",
            "169/169 [==============================] - 2s 10ms/step - loss: 0.2157 - accuracy: 0.9519 - val_loss: 1.2809 - val_accuracy: 0.6361\n",
            "Epoch 6/10\n",
            "169/169 [==============================] - 2s 14ms/step - loss: 0.1016 - accuracy: 0.9822 - val_loss: 1.3756 - val_accuracy: 0.6244\n",
            "Epoch 7/10\n",
            "169/169 [==============================] - 2s 11ms/step - loss: 0.0500 - accuracy: 0.9924 - val_loss: 1.4960 - val_accuracy: 0.6227\n",
            "Epoch 8/10\n",
            "169/169 [==============================] - 2s 10ms/step - loss: 0.0315 - accuracy: 0.9955 - val_loss: 1.5914 - val_accuracy: 0.6210\n",
            "Epoch 9/10\n",
            "169/169 [==============================] - 2s 10ms/step - loss: 0.0239 - accuracy: 0.9963 - val_loss: 1.6614 - val_accuracy: 0.6311\n",
            "Epoch 10/10\n",
            "169/169 [==============================] - 2s 10ms/step - loss: 0.0170 - accuracy: 0.9970 - val_loss: 1.7207 - val_accuracy: 0.6277\n",
            "187/187 [==============================] - 1s 3ms/step\n",
            "47/47 [==============================] - 0s 3ms/step\n",
            "Training Accuracy: 0.9612\n",
            "Test Accuracy: 0.5441\n"
          ]
        }
      ],
      "source": [
        "# ...\n",
        "\n",
        "# Tokenize the text and convert it to sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text for _, text in psychExp_txt])\n",
        "X_sequences = tokenizer.texts_to_sequences([text for _, text in psychExp_txt])\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "X_padded = pad_sequences(X_sequences)\n",
        "\n",
        "# Convert labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_all)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=123)\n",
        "\n",
        "# Build the CNN model\n",
        "embedding_dim = 50  # adjust as needed\n",
        "max_sequence_len = X_padded.shape[1]  # length of padded sequences\n",
        "num_classes = len(set(y_encoded))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_len))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "train_acc = accuracy_score(y_train, np.argmax(model.predict(X_train), axis=1))\n",
        "test_acc = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1))\n",
        "print(\"Training Accuracy: {:.4f}\".format(train_acc))\n",
        "print(\"Test Accuracy: {:.4f}\".format(test_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SPFx0C6jxub"
      },
      "outputs": [],
      "source": [
        "t1 = \"I love swimming, really enjoyed it!\"\n",
        "t2 = \"I hate you, you fucking disgusting\"\n",
        "t3 = \"I am afraid of thunder\"\n",
        "t4 = \"i am so angry, am very fed up with you\"\n",
        "t5 = \"My uncle died\"\n",
        "t6 = \"I enjoy being alive\"\n",
        "t7 = \"I am so sorry, i didnt realise this could be like this\"\n",
        "t8 =\" i am so happy\"\n",
        "texts = [t1, t2, t3, t4, t5, t6, t7,t8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhs6cI0fyLmW"
      },
      "source": [
        "# Pengujian dengan CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdNOHU6azUz-",
        "outputId": "c8c1257b-7083-4101-8123-9dadfa89aa48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 16ms/step\n",
            "😂 I love pork, really enjoyed it!\n",
            "😒 I hate you, you fucking disgusting\n",
            "😱 I am afraid of thunder\n",
            "😠 i am so angry, am very fed up with you\n",
            "😢 My uncle died\n",
            "😳 I enjoy being alive\n",
            "😳 I am so sorry, i didnt realise this could be like this\n"
          ]
        }
      ],
      "source": [
        "# Tokenize and pad the input texts\n",
        "text_sequences = tokenizer.texts_to_sequences(texts)\n",
        "text_padded_cnn = pad_sequences(text_sequences, maxlen=max_sequence_len)\n",
        "\n",
        "# Make predictions using the trained CNN model\n",
        "predictions_cnn = model.predict(text_padded_cnn)\n",
        "\n",
        "# Decode the predicted labels back to emotions\n",
        "predicted_labels_cnn = label_encoder.inverse_transform(np.argmax(predictions_cnn, axis=1))\n",
        "\n",
        "# Print the results\n",
        "for label, text in zip(predicted_labels_cnn, texts):\n",
        "    print(\"{} {}\".format(emoji_dict[label], text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL4wXkmxL8iO"
      },
      "source": [
        "# Training dengan LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qvv86p3G0lWb",
        "outputId": "0d22a489-55c1-4b4b-8d57-f10167f42235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "169/169 [==============================] - 17s 92ms/step - loss: 1.8953 - accuracy: 0.2362 - val_loss: 1.7436 - val_accuracy: 0.3339\n",
            "Epoch 2/10\n",
            "169/169 [==============================] - 14s 83ms/step - loss: 1.4563 - accuracy: 0.4847 - val_loss: 1.3490 - val_accuracy: 0.5225\n",
            "Epoch 3/10\n",
            "169/169 [==============================] - 14s 83ms/step - loss: 0.9808 - accuracy: 0.6615 - val_loss: 1.3348 - val_accuracy: 0.5426\n",
            "Epoch 4/10\n",
            "169/169 [==============================] - 14s 83ms/step - loss: 0.6657 - accuracy: 0.7876 - val_loss: 1.3202 - val_accuracy: 0.6010\n",
            "Epoch 5/10\n",
            "169/169 [==============================] - 14s 83ms/step - loss: 0.4569 - accuracy: 0.8546 - val_loss: 1.3439 - val_accuracy: 0.5860\n",
            "Epoch 6/10\n",
            "169/169 [==============================] - 14s 83ms/step - loss: 0.3195 - accuracy: 0.9027 - val_loss: 1.4403 - val_accuracy: 0.5793\n",
            "Epoch 7/10\n",
            "169/169 [==============================] - 15s 87ms/step - loss: 0.2358 - accuracy: 0.9296 - val_loss: 1.5664 - val_accuracy: 0.5793\n",
            "Epoch 8/10\n",
            "169/169 [==============================] - 14s 83ms/step - loss: 0.1776 - accuracy: 0.9515 - val_loss: 1.6883 - val_accuracy: 0.5760\n",
            "Epoch 9/10\n",
            "169/169 [==============================] - 14s 83ms/step - loss: 0.1246 - accuracy: 0.9692 - val_loss: 1.8933 - val_accuracy: 0.5810\n",
            "Epoch 10/10\n",
            "169/169 [==============================] - 14s 82ms/step - loss: 0.1105 - accuracy: 0.9695 - val_loss: 2.0310 - val_accuracy: 0.5492\n",
            "187/187 [==============================] - 5s 24ms/step\n",
            "47/47 [==============================] - 1s 21ms/step\n",
            "Training Accuracy: 0.9166\n",
            "Test Accuracy: 0.5000\n"
          ]
        }
      ],
      "source": [
        "# ...\n",
        "\n",
        "# Tokenize the text and convert it to sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text for _, text in psychExp_txt])\n",
        "X_sequences = tokenizer.texts_to_sequences([text for _, text in psychExp_txt])\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "X_padded = pad_sequences(X_sequences)\n",
        "\n",
        "# Convert labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_all)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=123)\n",
        "\n",
        "# Build the LSTM model\n",
        "embedding_dim = 50  # adjust as needed\n",
        "max_sequence_len = X_padded.shape[1]  # length of padded sequences\n",
        "num_classes = len(set(y_encoded))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_len))\n",
        "model.add(LSTM(100))  # You can adjust the number of LSTM units as needed\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "train_acc = accuracy_score(y_train, np.argmax(model.predict(X_train), axis=1))\n",
        "test_acc = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1))\n",
        "print(\"Training Accuracy: {:.4f}\".format(train_acc))\n",
        "print(\"Test Accuracy: {:.4f}\".format(test_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ofniIm3Omh2"
      },
      "source": [
        "# Hasil pengujian dengan LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZUfJ9vu1iX9",
        "outputId": "19142049-c3f1-4078-f682-72e0b987d7fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "😂 I love pork, really enjoyed it!\n",
            "😒 I hate you, you fucking disgusting\n",
            "😱 I am afraid of thunder\n",
            "😠 i am so angry, am very fed up with you\n",
            "😢 My uncle died\n",
            "😂 I enjoy being alive\n",
            "😢 I am so sorry, i didnt realise this could be like this\n"
          ]
        }
      ],
      "source": [
        "# Tokenize and pad the input texts\n",
        "text_sequences_lstm = tokenizer.texts_to_sequences(texts)\n",
        "text_padded_lstm = pad_sequences(text_sequences_lstm, maxlen=max_sequence_len)\n",
        "\n",
        "# Make predictions using the trained LSTM model\n",
        "predictions_lstm = model.predict(text_padded_lstm)\n",
        "\n",
        "# Decode the predicted labels back to emotions\n",
        "predicted_labels_lstm = label_encoder.inverse_transform(np.argmax(predictions_lstm, axis=1))\n",
        "\n",
        "# Print the results with emojis\n",
        "for label, text in zip(predicted_labels_lstm, texts):\n",
        "    print(\"{} {}\".format(emoji_dict[label], text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z17ObnvJMBwU"
      },
      "source": [
        "# Training dengan Char-Level CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnbVK0aJ1_X0",
        "outputId": "ba885151-7408-4b53-d686-705c1185e68f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "169/169 [==============================] - 6s 34ms/step - loss: 1.9380 - accuracy: 0.1790 - val_loss: 1.9096 - val_accuracy: 0.2404\n",
            "Epoch 2/10\n",
            "169/169 [==============================] - 6s 37ms/step - loss: 1.8250 - accuracy: 0.2867 - val_loss: 1.7292 - val_accuracy: 0.3439\n",
            "Epoch 3/10\n",
            "169/169 [==============================] - 6s 33ms/step - loss: 1.6415 - accuracy: 0.3931 - val_loss: 1.5947 - val_accuracy: 0.4107\n",
            "Epoch 4/10\n",
            "169/169 [==============================] - 6s 38ms/step - loss: 1.5244 - accuracy: 0.4397 - val_loss: 1.4847 - val_accuracy: 0.4741\n",
            "Epoch 5/10\n",
            "169/169 [==============================] - 6s 33ms/step - loss: 1.4414 - accuracy: 0.4715 - val_loss: 1.4391 - val_accuracy: 0.4858\n",
            "Epoch 6/10\n",
            "169/169 [==============================] - 6s 37ms/step - loss: 1.3735 - accuracy: 0.4994 - val_loss: 1.4718 - val_accuracy: 0.4674\n",
            "Epoch 7/10\n",
            "169/169 [==============================] - 6s 33ms/step - loss: 1.3397 - accuracy: 0.5192 - val_loss: 1.3960 - val_accuracy: 0.5092\n",
            "Epoch 8/10\n",
            "169/169 [==============================] - 6s 38ms/step - loss: 1.2942 - accuracy: 0.5305 - val_loss: 1.4430 - val_accuracy: 0.4875\n",
            "Epoch 9/10\n",
            "169/169 [==============================] - 6s 33ms/step - loss: 1.2631 - accuracy: 0.5382 - val_loss: 1.3599 - val_accuracy: 0.4925\n",
            "Epoch 10/10\n",
            "169/169 [==============================] - 6s 38ms/step - loss: 1.2238 - accuracy: 0.5573 - val_loss: 1.3744 - val_accuracy: 0.5042\n",
            "187/187 [==============================] - 2s 9ms/step\n",
            "47/47 [==============================] - 0s 9ms/step\n",
            "Training Accuracy: 0.5640\n",
            "Test Accuracy: 0.4820\n",
            "47/47 [==============================] - 0s 10ms/step\n",
            "😂 I love pork, really enjoyed it!\n",
            "😠 I hate you, you fucking disgusting\n",
            "😳 I am afraid of thunder\n",
            "😢 i am so angry, am very fed up with you\n",
            "😂 My uncle died\n",
            "😱 I enjoy being alive\n",
            "😢 I am so sorry, i didnt realise this could be like this\n"
          ]
        }
      ],
      "source": [
        "# ...\n",
        "\n",
        "# Tokenize the characters\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts([text for _, text in psychExp_txt])\n",
        "X_sequences = tokenizer.texts_to_sequences([text for _, text in psychExp_txt])\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "X_padded = pad_sequences(X_sequences)\n",
        "\n",
        "# Convert labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y_all)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=123)\n",
        "\n",
        "# Build the character-level CNN model\n",
        "embedding_dim = 50  # adjust as needed\n",
        "max_sequence_len = X_padded.shape[1]  # length of padded sequences\n",
        "num_classes = len(set(y_encoded))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_len))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "train_acc = accuracy_score(y_train, np.argmax(model.predict(X_train), axis=1))\n",
        "test_acc = accuracy_score(y_test, np.argmax(model.predict(X_test), axis=1))\n",
        "print(\"Training Accuracy: {:.4f}\".format(train_acc))\n",
        "print(\"Test Accuracy: {:.4f}\".format(test_acc))\n",
        "\n",
        "# ...\n",
        "\n",
        "# Make predictions using the trained model\n",
        "predictions_char_cnn = model.predict(X_test)\n",
        "\n",
        "# Decode the predicted labels back to emotions\n",
        "predicted_labels_char_cnn = label_encoder.inverse_transform(np.argmax(predictions_char_cnn, axis=1))\n",
        "\n",
        "# # Print the results\n",
        "for label, text in zip(predicted_labels_char_cnn, texts):\n",
        "    print(\"{} {}\".format(emoji_dict[label], text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FH9Q6OLOsCa"
      },
      "source": [
        "# Hasil pengujian dengan Character level CNN, CNN, LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_qHGYq53KRP",
        "outputId": "e1c110a3-9a4e-4862-800c-3b9ad44b44ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text                                               | CNN Output           | Char CNN Output      | LSTM Output         \n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "I love swimming, really enjoyed it!                | 😂 (joy)              | 😂 (joy)              | 😂 (joy)             \n",
            "I hate you, you fucking disgusting                 | 😒 (disgust)          | 😠 (anger)            | 😒 (disgust)         \n",
            "I am afraid of thunder                             | 😱 (fear)             | 😳 (guilt)            | 😱 (fear)            \n",
            "i am so angry, am very fed up with you             | 😠 (anger)            | 😢 (sadness)          | 😠 (anger)           \n",
            "My uncle died                                      | 😢 (sadness)          | 😂 (joy)              | 😢 (sadness)         \n",
            "I enjoy being alive                                | 😳 (guilt)            | 😱 (fear)             | 😂 (joy)             \n",
            "I am so sorry, i didnt realise this could be like this | 😳 (shame)            | 😢 (sadness)          | 😢 (sadness)         \n"
          ]
        }
      ],
      "source": [
        "# Print the results with emojis and model labels\n",
        "print(\"{:<50} | {:<20} | {:<20} | {:<20}\".format(\"Text\", \"CNN Output\", \"Char CNN Output\", \"LSTM Output\"))\n",
        "print(\"-\" * 120)\n",
        "\n",
        "for text, label_cnn, label_char_cnn, label_lstm in zip(texts, predicted_labels_cnn, predicted_labels_char_cnn, predicted_labels_lstm):\n",
        "    print(\"{:<50} | {:<20} | {:<20} | {:<20}\".format(text, f\"{emoji_dict[label_cnn]} ({label_cnn})\", f\"{emoji_dict[label_char_cnn]} ({label_char_cnn})\", f\"{emoji_dict[label_lstm]} ({label_lstm})\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eGrExyQtMQko",
        "YvV8ALkDMk_8",
        "kUY7jCsCkD4H"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
